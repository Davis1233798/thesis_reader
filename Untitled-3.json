"[1]: https://arxiv.org/abs/2302.00923 \"\"\n[2]: https://arxiv.org/pdf/2302.00923.pdf \"\"\n[3]: https://blog.csdn.net/weixin_38506372/article/details/129345761 \"\"\n[4]: https://owl.purdue.edu/owl/research_and_citation/apa_style/apa_formatting_and_style_guide/index.html \"\"\n[5]: https://www.scribbr.com/apa-style/format/ \"\"\n[6]: https://bing.com/search?q=APA7+citation+format \"\"\n[7]: https://www.scribbr.com/apa-style/in-text-citation/ \"\"\n[8]: https://owl.purdue.edu/owl/research_and_citation/apa_style/apa_formatting_and_style_guide/in_text_citations_the_basics.html \"\"\n[9]: https://www.easybib.com/guides/citation-guides/apa-format/apa-citation/ \"\"\n[10]: https://doi.org/10.48550/arXiv.2302.00923 \"\"\n[11]: https://github.com/amazon-science/mm-cot \"\"\n\n你好，這是Bing。我會嘗試回答你的問題。以下是我根據你給的文獻連結[^1^][1]和APA7引用格式[^2^][4] [^3^][5] [^4^][6] [^5^][7] [^6^][8] [^7^][9]整理出來的資訊：\n\n- 文獻名稱：Multimodal Chain-of-Thought Reasoning in Language Models\n- APA7引用格式：Zhang, Z., Zhang, A., Li, M., Zhao, H., Karypis, G., & Smola, A. (2023). Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923.\n- 文獻連結：[arXiv:2302.00923](https://arxiv.org/abs/2302.00923)\n- 這篇文獻的研究方法：這篇文獻提出了一種稱為Multimodal-CoT的方法，它將語言（文字）和視覺（圖像）兩種模態結合起來，並且分成兩個階段：理由生成和答案推理。理由生成階段利用大型語言模型（LLM）根據問題和上下文生成中間推理步驟（理由），答案推理階段利用一個小型的多模態模型根據理由和選項選擇最佳答案。\n- 這篇文獻對提示詞的量化方法：這篇文獻使用了一種基於模板的提示詞生成方法，它根據不同類型的問題和上下文動態生成適合的提示詞。提示詞的格式如下：\n\n    ```\n    Input\n    Language\n    <Question>\n    Context\n    <Context>\n    Vision\n    <Image>\n    Output\n    Rationale\n    <Rationale>\n    Answer\n    The answer is <Answer>.\n    ```\n- 這篇文獻的實驗步驟：\n    - 首先，作者從ScienceQA數據集中選取了包含圖像的問題，並將其分成訓練集、驗證集和測試集。\n    - 其次，作者使用GPT-3作為LLM，並使用不同大小的參數來生成理由。\n    - 然後，作者使用一個基於BERT的多模態模型作為答案推理模型，並使用訓練集來微調它。\n    - 最後，作者使用驗證集來選擇最佳的LLM和答案推理模型，並使用測試集來評估它們的性能。\n- 這篇文獻的研究成果：這篇文獻的主要成果是：\n    - 提出了一種新穎的多模態鏈式思考推理方法，可以有效地利用不同模態的信息來解決複雜的科學問題。\n    - 在ScienceQA數據集上實現了顯著的性能提升，超越了之前最先進的LLM（GPT-3.5）和人類表現。\n"